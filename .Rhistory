jitter(plot(galton))
lon <- read.csv(file="htpp://jakeporway.com/teaching/data/longevity.csv", as.is=T, header=T)
lon <- read.csv(file="http://jakeporway.com/teaching/data/longevity.csv", as.is=T, header=T)
hea(long)
head(lon)
hist(lon)
hist(long$AgeAtDeath
)
hist(lon$AgeAtDeath)
non.smokers <- lon[long$Smokes == 0,]
non.smokers <- lon[lon$Smokes == 0, ]
hist(non.smokers)
hist(non.smokers$AgeAtDeath)
par(new=T)
hist(lon[ lon$Smokes == 1,]$AgeAtDeath)
hist(non.smokers$AgeAtDeath)
hist(lon[ lon$Smokes == 1,]$AgeAtDeath)
smoker <- lon[ lon$Smokes == 1, ]
mean(smoker$AgeAtDeath)
mean(non.smoker$AgeAtDeath)
mean(non.smokers$AgeAtDeath)
lm(lon)
plot(lm(lon))
?square.error
?mse
?mse()
?mse
library('mse')
lm(galton$child ~ galton$parent)
plot(lm(galton$child ~ galton$parent))
linear.model(galton$child ~ galton$parent)
install.prackages('randomForest')
install.packages('randomForest')
install.packages('rpart')
snf <- read.csv(file="http://jakeporway.com/teaching/data/snf_4.csv", as.is=T, h=T)
head(snf)
names(snf)
max(time)
head(snf$times)
head(snf$time)
head(rev(sort(snf$time)))
min(snf$age)
head(sort(snf$age))
head(sort(snf$age), max=20)
head(sort(snf$age), limit=20)
sort((snf$age))[1:20]
sort((snf$age))[1:50]
sort((snf$age))[1:100]
sort((snf$age))[1:200]
head(snf)
snf$height <- (snf$feet * 12) + snf$inches
head(snf)
hist(snf$height, breaks=500)
hist(snf$weight, breaks=500)
hist(snf$weight, breaks=100)
hist(snf$weight, breaks=100, main="Histogram of Weight")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$height, breaks=100, main="Histogram of Heights", xlab="Height (in inches)")
?part
?par
par(mfrow=c(4,1))
hist(snf$height, breaks=100, main="Histogram of Heights", xlab="Height (in inches)")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$period_obs, breaks=100)
hist(snf$period_obs, breaks=100)
snf <- read.csv(file="http://jakeporway.com/teaching/data/snf_4.csv", as.is=T, h=T)
hist(snf$period_obs, breaks=100)
hist(snf$period_obs, breaks=500)
per.truc <- snf$period_obs[snf$period_obs < 200,]
head(snf$period_obs)
head(rev(sort(snf$period_obs))
)
hist(snf$period_stop, breaks=500)
per.truc <- snf[snf$period_obs < 200,]
hist(per.trunc$period_obs)
hist(per.truc$period_obs)
hist(per.truc$period_obs, breaks=100)
snf <- read.csv(file="http://jakeporway.com/teaching/data/snf_4.csv", as.is=T, h=T)
head(snf)
snf$height <- (snf$feet * 12) + snf$inches
head(snf$height)
par(mfrow=c(4,1))
hist(snf$height, breaks=100, main="Histogram of Heights", xlab="Height (in inches)")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$period_obs, breaks=500)
hist(snf$period_stop, breaks=500)
first.50.obs <- snf[snf$period_obs < 50,]
first.50.stop <- snf[snf$period_stop < 50,]
par(mfrow=c(4,1))
hist(snf$height, breaks=100, main="Histogram of Heights", xlab="Height (in inches)")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$period_obs, breaks=500)
hist(snf$period_stop, breaks=500)
lessthan.40.obs <- snf[snf$period_obs < 40,]
lessthan.40.stop <- snf[snf$period_stop < 40,]
jittered(scatterplot(lessthan.40.obs, lessthan.40.stop))
plot(jittered(lessthan.40.obs), lessthan.40.stop)
plot(jitter(lessthan.40.obs), lessthan.40.stop)
plot(jitter(lessthan.40.obs$period_obs), lessthan.40.stop$period_stop)
dim(lessthan.40.stop$period_stop
)
dim(lessthan.40.stop$period_stop)
head(lessthan.40.stop$period_stop)
type(lessthan.40.stop$period_stop)
typeof(lessthan.40.stop$period_stop)
max(lessthan.40.stop$period_stop)
min(lessthan.40.stop$period_stop)
dim(as.vector(lessthan.40.stop$period_stop))
dim(as.table(lessthan.40.stop$period_stop))
dim(as.data.frame(lessthan.40.stop$period_stop))
dim(snf)
lessthan.40 <- snf[snf$period.obs < 40,]
dim(lessthan.40)
max(snf[snf$period_obs])
max(snf[snf$period_obs,])
lessthan.40 <- snf[snf$period_obs < 40,]
dime(lessthan.40)
dim(lessthan.40)
plot(jitter(lessthan.40$period_obs), lessthan40$period_stop)
plot(jitter(lessthan.40$period_obs), lessthan.40$period_stop)
max(lessthn.40$period_stop)
max(lessthan.40$period_stop)
lessthan.40 <- snf[(snf$period_obs & snf$period_stop) < 40 ]
lessthan.40 <- snf[(snf$period_obs & snf$period_stop) < 40, ]
lessthan.40 <- snf[(snf$period_obs & snf$period_stop) < 40, ]
dim(lessthan.40)
max(lessthan.40$period_sto)
max(lessthan.40$period_stop)
lessthan.40 <- snf[snf$period_obs < 40 & snf$period_stop < 40, ]
dim(lessthan.40)
max(lesstan.40$period_stop)
max(lessthan.40$period_stop)
max(lessthan.40$period_obs)
ltfobs <- lessthan.40$period_obs
ltfst <- lessthan.40$period_stop
plot(jitter(ltfobs), ltfst)
plot(jitter(ltfobs), ltfst)
plot(jitter(ltfobs), ltfst, Main="Scatterplot of Time Observed vs Time Stopped", xlab="Time Observed", ylab="Time Stopped")
plot(jitter(ltfobs), ltfst, main="Scatterplot of Time Observed vs Time Stopped", xlab="Time Observed", ylab="Time Stopped")
lm(ltfobs, ltfst)
linear.model(ltfsobs, ltfst)
?lm
lm(ltfst ~ ltfobs)
linear.model <- lm(ltfst ~ ltfobs)
summar(linear.model)
summary(linear.model)
abline(linear.model)
slope <- linear.model$coefficients[[2]]
intercept <- linear.model$coefficients[[1]]
obs.time <- 5
prediction.5mins <- obs.time*slope + intercepts
prediction.5mins <- obs.time*slope + intercept
prediction.5mins
obs.time <- 60
prediction <- obs.time*slope + intercept
prediction
prediction.5mins <- obs.time*slope + intercept
obs.time <- 5
prediction <- obs.time*slope + intercept
prediction
obs.time <- 60
prediction
prediction <- obs.time*slope + intercept
prediction
summary(linear.model)
max(snf$height)
max(snf$height)
height <- snf$height
max(height)
95/12
.916667 * 12
weight <- snf$weight
max(weight)
plot(weight)
hist(weight)
hist(weight, breaks=100)
min(weight)
min(height)
reasonable <- snf[snf$height < 84 & (snf$weight > 80 & snf$weight < 400), ]
reasonable.weight <- reasonable$weight
reasonable.height <- reasonable$height
plot(jitter(reasonable$weight), reasonable$height, main="Scatterplot SNF Heights vs Weights", xlab="Weight", ylab="Height")
linear.model <- (reasonalbe$weight ~ reasonable$height)
abline(linear.model)
linear.model <- (reasonalbe$weight ~ reasonable$height)
summary(linear.model)
linear.model <- lm(reasonalbe$weight ~ reasonable$height)
linear.model <- lm(reasonable$weight ~ reasonableheight)
linear.model <- lm(reasonable$weight ~ reasonable$height)
abline(linear.model)
summary(linear.model)
linear.model
linear.model <- lm(reasonable$weight ~ reasonable$height)
lm <-lm(reasonable$weight ~ reasonable$height)
lm
lm <-lm(reasonable$height ~ reasonable$weight)
lm
abline(lm)
slope <- lm$coefficients[[2]]
intercept <- lm$coefficients[[1]]
obs.height <- 72
prediction <- obs.height*slope + intercept
prediction
lm <-lm(reasonable$weight ~ reasonable$height)
slope <- lm$coefficients[[2]]
intercept <- lm$coefficients[[1]]
prediction <- obs.height*slope + intercept
prediction
install.package(randomForest)
package.install(randomForest)
install.packages(randomForest)
install.packages('randomForest'')
install.packages('randomForest')
install.packages(randomForest)
library(randomForest)
?randomForest
head(snf)
fit <- randomForest(arrested ~ frisked + precinct + sex + race + age + period_obs + period_stop + period_obs + height + weight)
fit <- randomForest(snf$arrested ~ snf$frisked + snf$searched + snf$precinct + snf$sex + snf$race + snf$age + snf$period_obs + snf$period_stop + snf$period_obs + snf$height + snf$weight, data=snf)
print(fit)
fit <- randomForest(snf$arrested ~ snf$frisked + snf$searched + snf$precinct + snf$sex + snf$race + snf$age + snf$period_obs + snf$period_stop + snf$period_obs + snf$height + snf$weight, data=snf)
fit <- randomForest(snf$arrested ~ frisked + searched + precinct + sex + race + age + period_obs + period_stop + period_obs + height + weight, data=snf)
fit <- randomForest(arrested ~ frisked + searched + precinct + sex + race + age + period_obs + period_stop + period_obs + height + weight, data=snf)
pairs(snf)
library(rpart)
fit <- rpart(arrested ~ frisked + searched + precinct + sex + race + age + period_obs + period_stop + period_obs + height + weight, data=snf, method="class")
plot(fit)
printcp(fit)
plot(fit, uniform=TRUE,
main="Classification Tree for snf")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
tweets <- read.csv(file="~/Desktop/ccs/balacera/assets/balacera.csv")
text <- tweet$text
text <- tweets$text
typeof(text)
head(text)
text.corp <- Corpus(VectorSource(text))
lib(tm)
library(tm)
text.corp <- Corpus(VectorSource(text))
dim(text.corp)
head(text.corp)
text.corp[1:10]
text <- text.corp[1:100]
test <- text.corp[1:100]
test <- tm_map(test, stripWhitespace)
test <- tm_map(test, tolower)
test <- tm_map(test, removeWords, stopwords("spanish"))
test[1]
dtm <- DocumentTermMatrix(test, control = params)
dtm <- DocumentTermMatrix(test)
head(dtm)
dtm.mat <- as.matrix(dtm)
d <- dist(dtm.mat, method="euclidian")
fit <- hclust(d, method="Ward")
fit <- hclust(d, method="ward")
plot(fit)
test <- text.corp[1:1000]
test <- tm_map(test, stripWhitespace)
test <- tm_map(test, tolower)
test <- tm_map(test, removeWords, stopwords("spanish"))
#turn into DocumentTermMatrix
dtm <- DocumentTermMatrix(test)
dtm.mat <- as.matrix(dtm)
d <- dist(dtm.mat, method="euclidian")
fit <- hclust(d, method="ward")
plot(fit)
library(tm)
tweets <- read.csv(file="~/Desktop/ccs/balacera/assets/data/balacera.csv")
stopwords <- c(stopwords('spanish'), 'rt')
#establish the corpus
text.corp <- Corpus(VectorSource(tweets$text))
#preprocess the corpus
test <- text.corp[1:1000]
test <- tm_map(test, stripWhitespace)
test <- tm_map(test, tolower)
test <- tm_map(test, removePunctuation)
test <- tm_map(test, removeWords, stopwords)
dtm <- DocumentTermMatrix(test)
dtm2 <- removeSparseTerms(dtm, sparse=0.95)
df <- as.data.frame(inspect(dtm2))
df.scale <- scale(df)
#find euclidian distance
d <- dist(df.scale, method = "euclidean")
fit <- hclust(d, method="ward")
plot(fit)
groups <- cutree(fit, k=7)
rect.hclust(fit, k=5, border="red")
clusters <- rect.hclust(fit, k=5, border="red")
cluster.df <- as.data.frame((clusters[[1]]))
cluster.vec <- as.vector(cluster.df[,])
inspect(cluster[cluster.vec])
inspect(clusters[cluster.vec])
inspect(dtm[cluster.vec])
inspect(text.corp[cluster.vec])
findFreqTerms(text.corp[cluster.vec], lowfreq=30)
findFreqTerms(dtm[cluster.vec], lowfreq=30)
findFreqTerms(dtm[cluster.vec], lowfreq=10)
cluster.vec <- as.vector(cluster.df[,])
findFreqTerms(text.corp[cluster.vec], lowfreq=30)
cluster.df <- as.data.frame((clusters[[2]]))
cluster.vec <- as.vector(cluster.df[,])
#inspect tweets
inspect(cluster[cluster.vec])
cluster.df <- as.data.frame((clusters[[2]]))
cluster.vec <- as.vector(cluster.df[,])
#inspect tweets
inspect(text.corp[cluster.vec])
cluster.df <- as.data.frame((clusters[[3]]))
cluster.vec <- as.vector(cluster.df[,])
#inspect tweets
inspect(text.corp[cluster.vec])
cluster.df <- as.data.frame((clusters[[4]]))
cluster.vec <- as.vector(cluster.df[,])
#inspect tweets
inspect(text.corp[cluster.vec])
cities.pattern <- "Ju[aá]rez|Culiac[aá]n|Tijuana|Chihuahua|Acapulco\s+de\s+Ju[aá]rez|G[oó]mez|Palacio|Torre[oó]n|Mazatl[aá]n|Nogales|Durango|Navolato|Monterrey|Morelia|Ahome|Tepic|Reynosa|Guasave|Hidalgo\s+del\s+Parral|Ecatepec\s+de\s+Morelos|Uruapan"
# cities.pattern <- "Ju[aá]rez|Culiac[aá]n|Tijuana|Chihuahua|Acapulco\\s+de\\s+Ju[aá]rez|G[oó]mez|Palacio|Torre[oó]n|Mazatl[aá]n|Nogales|Durango|Navolato|Monterrey|Morelia|Ahome|Tepic|Reynosa|Guasave|Hidalgo\\s+del\\s+Parral|Ecatepec\\s+de\\s+Morelos|Uruapan"
# cities.pattern <- "Ju[aá]rez|Culiac[aá]n|Tijuana|Chihuahua|Acapulco\\s+de\\s+Ju[aá]rez|G[oó]mez|Palacio|Torre[oó]n|Mazatl[aá]n|Nogales|Durango|Navolato|Monterrey|Morelia|Ahome|Tepic|Reynosa|Guasave|Hidalgo\\s+del\\s+Parral|Ecatepec\\s+de\\s+Morelos|Uruapan/i"
?grep
?tm_filter
tm_filter(text.corpus, cities.pattern)
tm_filter(text.corp, cities.pattern)
cities.pattern <- "Ju[aá]rez|Culiac[aá]n|Tijuana|Chihuahua|Acapulco\\s+de\\s+Ju[aá]rez|G[oó]mez|Palacio|Torre[oó]n|Mazatl[aá]n|Nogales|Durango|Navolato|Monterrey|Morelia|Ahome|Tepic|Reynosa|Guasave|Hidalgo\\s+del\\s+Parral|Ecatepec\\s+de\\s+Morelos|Uruapan/i"
tm_filter(text.corpus, cities.pattern)
tm_filter(text.corp, cities.pattern)
cities <- tm_filter(text.corp, cities.pattern, ignore.case=TRUE)
cities <- tm_filter(text.corp, grep(cities.pattern, text.corp, ignore.case=TRUE))
CITIES
cities
text.corp
inspect(cities[1:20])
stopwords <- c(stopwords('spanish'), 'rt')
cities <- tm_map(cities, removeWords, stopwords)
cities <- tm_map(cities, removePunctuation)
cities <- tm_map(cities, tolower)
cities <- tm_map(cities, stripWhitespace)
cities.dtm <- DocumentTermMatrix(cities)
cities.dtm <- removeSparseTerms(cities.dtm, sparse=0.95)
citites.df <- as.data.frame(inspect(cities.dtm))
cities.scale <- scale(cities.df)
cities.scale <- scale(citites.df)
cities.d <- dist(cities.scale, method="euclidian")
cities.fit <- hclust(cities.d, method="ward")
plot(cities.fit)
cities.groups <- cutree(cities.fit, k=7)
clusters <- rect.hclust(fit, k=7, border="red")
rect.hclust(fit, k=7, border="red")
cluster.df <- as.data.frame((clusters[[1]]))
cluster.vec <- as.vector(cluster.df[,])
inspect(cluster[cluster.vec])
inspect(cities[cluster.vec])
patter <- "Ju[aá]rez""
patter <- "Ju[aá]rez"
pattern <- "Ju[aá]rez"
cities <- tm_filter(cities, grep(pattern, cities, ignore.case=TRUE))
cities
cities <- tm_filter(cities, grep("reynosa", cities, ignore.case=TRUE))
cities
cluster.df <- as.data.frame((clusters[[2]]))
cluster.vec <- as.vector(cluster.df[,])
inspect(cities[cluster.vec])
hist(tweets$created_at_seconds, breaks=1000, xlab="time", main="Histogram of Tweet Creation Time")
tweets <- read.csv(file="~/Desktop/ccs/balacera/assets/data/balacera-nov.csv")
hist(tweets$created_at_seconds, breaks=1000, xlab="time", main="Histogram of Tweet Creation Time")
sleep <- read.csv("~/Desktop/visuazing_js/week_10/sleep.csv")
head(sleep)
plot(sleep$timeInDeep, type="bar")
plot(sleep$timeInDeep, type="b")
plot(sleep$timeInDeep, type="l")
survey <- read.csv("Survey+Response.csv")
clean_data <- function(survey){
names(survey) <- c("waitlist","program","tools","exp.Rmodeling","b5","b6","b7","b8","b9","b10","b11","gender","primaryeditor","exp.Rgraphics","exp.Radvanced","exp.documentation","exp.Matlab","exp.Github","b19","b20","b21","b22","b23","b24","b25","b26","b27","b28","b29","b30","b31","b32","b33","b34","b35","b36","b37","b38")
survey <- survey[,c(-5:-11,-19:-38)]
#dummy variables for each language/tool in tools
tooldummies = c()
toolList <- c("Github","Excel","SQL","RStudio","ggplot2","shell", "C/C","Python","LaTeX","(grep)","Sweave/knitr","XML","Web: html css js","dropbox","google drive","SPSS","Stata")
for(t in toolList){
tooldummies <- cbind(tooldummies,grepl(t,survey$tools))
}
tooldummies <- cbind(tooldummies,(grepl("R,",survey$tools)==TRUE | (grepl("R",survey$tools)==TRUE & grepl("RStudio",survey$tools)==FALSE)))
colnames(tooldummies) <- c("GitHub","Excel","SQL","RStudio","ggplot2","shell", "C","Python","LaTeX","grep","Sweave","XML","Web","dropbox","googledrive","SPSS","Stata","R")
survey <- cbind(survey,tooldummies)
levels(survey$gender)
levels(survey$gender) <- c("doesn't matter", "doesn't matter", "he/him", "she/her")
levels(survey$gender) <- c("Unknown", "Male", "Female")
survey = survey %>% mutate(primaryeditor = factor(
ifelse(grepl("[Aa]tom",primaryeditor),
"Atom",
ifelse(grepl("[Ss]ublime",primaryeditor ),
"Sublime Text",
ifelse(grepl("[Vv]i",primaryeditor),
"vi/vim",
ifelse(grepl("[Ww]rangler",primaryeditor ),
"TextWrangler",
ifelse(grepl("[Rr][Ss]tudio",primaryeditor ),
"Rstudio",
ifelse(grepl("[Pp]ython | jupyter | ipynb",primaryeditor ),
"iPython",
ifelse(grepl("[Mm]ate", primaryeditor ),
"TextMate",
ifelse(grepl("[Ww]ebstorm", primaryeditor ),
"Webstorm",
ifelse(grepl("[Xx]code", primaryeditor ),
"Xcode",
ifelse(grepl("[Ss]tata", primaryeditor ),
"Stata",
ifelse(grepl("[Nn]otepad", primaryeditor ),
"notepad++", "Other"
)
)
)
)
)
)
)
)
)
)
)), number_tools = sapply(strsplit(as.character(tools),","), length)
)
# levels(survey$program) = c("Applied Math", "MS IDSE", "Cert IDSE",
#                            "MS IDSE","MS IDSE","MS IDSE",
#                            "Other", "Other", "PhD. BMI",
#                            "QMSS", "QMSS","MS Stat")
levels(survey$program) = c("Other Masters", "MS IDSE", "Cert IDSE",
"MS IDSE","MS IDSE","MS IDSE",
"Other Masters", "Other PhD", "Other PhD",
"QMSS", "QMSS","MS Stat")
return(survey)
}
clean_data(survey)
reorder_size <- function(x) {
factor(x, levels = names(sort(table(x), decreasing = TRUE)))
}
clean_data <- function(survey){
names(survey) <- c("waitlist","program","tools","exp.Rmodeling","b5","b6","b7","b8","b9","b10","b11","gender","primaryeditor","exp.Rgraphics","exp.Radvanced","exp.documentation","exp.Matlab","exp.Github","b19","b20","b21","b22","b23","b24","b25","b26","b27","b28","b29","b30","b31","b32","b33","b34","b35","b36","b37","b38")
survey <- survey[,c(-5:-11,-19:-38)]
#dummy variables for each language/tool in tools
tooldummies = c()
toolList <- c("Github","Excel","SQL","RStudio","ggplot2","shell", "C/C","Python","LaTeX","(grep)","Sweave/knitr","XML","Web: html css js","dropbox","google drive","SPSS","Stata")
for(t in toolList){
tooldummies <- cbind(tooldummies,grepl(t,survey$tools))
}
tooldummies <- cbind(tooldummies,(grepl("R,",survey$tools)==TRUE | (grepl("R",survey$tools)==TRUE & grepl("RStudio",survey$tools)==FALSE)))
colnames(tooldummies) <- c("GitHub","Excel","SQL","RStudio","ggplot2","shell", "C","Python","LaTeX","grep","Sweave","XML","Web","dropbox","googledrive","SPSS","Stata","R")
survey <- cbind(survey,tooldummies)
levels(survey$gender)
levels(survey$gender) <- c("doesn't matter", "doesn't matter", "he/him", "she/her")
levels(survey$gender) <- c("Unknown", "Male", "Female")
survey = survey %>% mutate(primaryeditor = factor(
ifelse(grepl("[Aa]tom",primaryeditor),
"Atom",
ifelse(grepl("[Ss]ublime",primaryeditor ),
"Sublime Text",
ifelse(grepl("[Vv]i",primaryeditor),
"vi/vim",
ifelse(grepl("[Ww]rangler",primaryeditor ),
"TextWrangler",
ifelse(grepl("[Rr][Ss]tudio",primaryeditor ),
"Rstudio",
ifelse(grepl("[Pp]ython | jupyter | ipynb",primaryeditor ),
"iPython",
ifelse(grepl("[Mm]ate", primaryeditor ),
"TextMate",
ifelse(grepl("[Ww]ebstorm", primaryeditor ),
"Webstorm",
ifelse(grepl("[Xx]code", primaryeditor ),
"Xcode",
ifelse(grepl("[Ss]tata", primaryeditor ),
"Stata",
ifelse(grepl("[Nn]otepad", primaryeditor ),
"notepad++", "Other"
)
)
)
)
)
)
)
)
)
)
)), number_tools = sapply(strsplit(as.character(tools),","), length)
)
# levels(survey$program) = c("Applied Math", "MS IDSE", "Cert IDSE",
#                            "MS IDSE","MS IDSE","MS IDSE",
#                            "Other", "Other", "PhD. BMI",
#                            "QMSS", "QMSS","MS Stat")
levels(survey$program) = c("Other Masters", "MS IDSE", "Cert IDSE",
"MS IDSE","MS IDSE","MS IDSE",
"Other Masters", "Other PhD", "Other PhD",
"QMSS", "QMSS","MS Stat")
return(survey)
}
give.n <- function(x){
return(c(y = mean(x), label = length(x)))
}
clean_data(survey)
survey
survey <- read.csv("Survey+Response.csv")
pwd()
setwd("/Users/pdarche/Desktop/qmss/spring2016/Stat4701/EDAV_HW1/")
survey <- read.csv("Survey+Response.csv")
survey <- read.csv("Survey+Response.csv")
survey
clean_data(survey)
install.packages('dplyer')
install.packages('dplyr')
clean_data(survey)
install.packages('dplyr')
