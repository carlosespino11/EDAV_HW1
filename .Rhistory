h <- hist(tw$seconds, breaks=500)$counts
head(h)
plot(h, type="l")
acf(h)
acf(h, lag.max = 200)
a <-acf(h, lag.max = 200)
names(a)
rev(order(a$acf))
count <- ts(h$counts, frequency=180)
count <- ts(h, frequency=180)
parts <- decompose(count)
plot(parts)
snf <- read.csv(file="~/Desktop/dwob/week_7/snf_3.csv", as.is=T, h=T)
head(snf)
snf$xy <- paste(snf$x, ",", snf$y, sep="")
head(snf)
geo <- read.csv(file="~/Desktop/dwob/week_7/geo.csv", as.is=T, h=T)
head(geo)
dim(geo)
geo$xy <- paste(geo$xcoord, ",", geo$ycoord, sep="")
head(geo)
snf.geo <- merge(snf, geo, by="xy")
head(snf.geo)
dim(snf.geo)
dim(snf)
dim(geo)
?dim
library('map')
library('maps')
map('county', 'new york',
xlim=c(-74.25026, -73.70196),
ylim=c(40.50553, 40.91289),
mar=c(0,0,0,0))
?points
head(snf.geo)
points(snf.geo$lon, snf.geo$lat)
points(snf.geo$lon, snf.geo$lat, col=2, pch=19, cex=0.8)
points(snf.geo$lon, snf.geo$lat, col=2, pch=19, cex=0.1)
points(snf.geo$lon, snf.geo$lat, col=2, pch=19, cex=0.1)
map('county', 'new york', xlim=c(-74.25026, -73.70196), ylim=c(40.50553, 40.91289), mar=c(0,0,0,0))
points(snf.geo$lon, snf.geo$lat, col=2, pch=19, cex=0.1)
map('county', 'new york', xlim=c(-74.25026, -73.70196), ylim=c(40.50553, 40.91289), mar=c(0,0,0,0))
points(snf.geo$lon, snf.geo$lat, rgb(.9, 0, 0, .2), pch=19, cex=0.1)
points(snf.geo$lon, snf.geo$lat, col=rgb(.9, 0, 0, .2), pch=19, cex=0.1)
map('county', 'new york', xlim=c(-74.25026, -73.70196), ylim=c(40.50553, 40.91289), mar=c(0,0,0,0))
points(snf.geo$lon, snf.geo$lat, col=rgb(.9, 0, 0, .2), pch=19, cex=0.3)
head(snf.geo)
head(snf.geo$race)
snf.geo$race <- as.factor(snf.geo$race)
snf.geo$race <- as.numeric(snf.geo$race)
head(snf.geo$race)
unique(snf$race)
unique(snf.geo$race)
map('county', 'new york', xlim=c(-74.25026, -73.70196), ylim=c(40.50553, 40.91289), mar=c(0,0,0,0))
points(snf.geo$lon, snf.geo$lat, col=snf.geo$race, pch=19, cex=0.3)
?as.data.frame
?data.frame()
snf.geo$race <- as.factor(snf.geo$race)
unique(snf.geo$race)
snf.geo$race <- as.character(snf.geo$race)
unique(snf.geo$race)
library('animation')
head(snf.geo)
snf.geo$time <- as.POSIXlt(snf.geo$time)
head(snf.geo$time)
packages.install('lubridate')
install.packages('lubridate')
library('lubridate')
?mday
head(mday(snf.geo$time))
head(snf.geo$time)
time <- mday(snf.geo$time)
head(time)
snf$geo.time <- mday(snf.geo$time)
snf.geo$time <- mday(snf.geo$time)
head(snf.geo)
ith.day <- snf.geo[snf.geo$time == 10,]
head(ith.day)
saveHTML({
for (i in 1:30) {
map('county', 'new york', xlim=c(-74.25026, -73.70196), ylim=c(40.50553, 40.91289), mar=c(0,0,0,0))
ith.day <- snf.geo[snf.geo$time == i,]
points(ith.day$lon, ith.day$lat, col=2, pch=18, cex=0.8)
ani.pause()
}
})
install.packages('psych')
library('psych')
?psych
?galton
head(galton)
hist(galton)
names(galton)
hist(children)
hist(child)
head(galton$child)
hist(galton$child)
hist(galton$child, breaks=100)
lmod(galton)
ls(galton)
lm(galton)
plot(lm(galton))
plot(galton)
plot(lm(galton))
?jitter
j <- jitter(galton)
jitter(plot(galton))
lon <- read.csv(file="htpp://jakeporway.com/teaching/data/longevity.csv", as.is=T, header=T)
lon <- read.csv(file="http://jakeporway.com/teaching/data/longevity.csv", as.is=T, header=T)
hea(long)
head(lon)
hist(lon)
hist(long$AgeAtDeath
)
hist(lon$AgeAtDeath)
non.smokers <- lon[long$Smokes == 0,]
non.smokers <- lon[lon$Smokes == 0, ]
hist(non.smokers)
hist(non.smokers$AgeAtDeath)
par(new=T)
hist(lon[ lon$Smokes == 1,]$AgeAtDeath)
hist(non.smokers$AgeAtDeath)
hist(lon[ lon$Smokes == 1,]$AgeAtDeath)
smoker <- lon[ lon$Smokes == 1, ]
mean(smoker$AgeAtDeath)
mean(non.smoker$AgeAtDeath)
mean(non.smokers$AgeAtDeath)
lm(lon)
plot(lm(lon))
?square.error
?mse
?mse()
?mse
library('mse')
lm(galton$child ~ galton$parent)
plot(lm(galton$child ~ galton$parent))
linear.model(galton$child ~ galton$parent)
install.prackages('randomForest')
install.packages('randomForest')
install.packages('rpart')
snf <- read.csv(file="http://jakeporway.com/teaching/data/snf_4.csv", as.is=T, h=T)
head(snf)
names(snf)
max(time)
head(snf$times)
head(snf$time)
head(rev(sort(snf$time)))
min(snf$age)
head(sort(snf$age))
head(sort(snf$age), max=20)
head(sort(snf$age), limit=20)
sort((snf$age))[1:20]
sort((snf$age))[1:50]
sort((snf$age))[1:100]
sort((snf$age))[1:200]
head(snf)
snf$height <- (snf$feet * 12) + snf$inches
head(snf)
hist(snf$height, breaks=500)
hist(snf$weight, breaks=500)
hist(snf$weight, breaks=100)
hist(snf$weight, breaks=100, main="Histogram of Weight")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$height, breaks=100, main="Histogram of Heights", xlab="Height (in inches)")
?part
?par
par(mfrow=c(4,1))
hist(snf$height, breaks=100, main="Histogram of Heights", xlab="Height (in inches)")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$period_obs, breaks=100)
hist(snf$period_obs, breaks=100)
snf <- read.csv(file="http://jakeporway.com/teaching/data/snf_4.csv", as.is=T, h=T)
hist(snf$period_obs, breaks=100)
hist(snf$period_obs, breaks=500)
per.truc <- snf$period_obs[snf$period_obs < 200,]
head(snf$period_obs)
head(rev(sort(snf$period_obs))
)
hist(snf$period_stop, breaks=500)
per.truc <- snf[snf$period_obs < 200,]
hist(per.trunc$period_obs)
hist(per.truc$period_obs)
hist(per.truc$period_obs, breaks=100)
snf <- read.csv(file="http://jakeporway.com/teaching/data/snf_4.csv", as.is=T, h=T)
head(snf)
snf$height <- (snf$feet * 12) + snf$inches
head(snf$height)
par(mfrow=c(4,1))
hist(snf$height, breaks=100, main="Histogram of Heights", xlab="Height (in inches)")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$period_obs, breaks=500)
hist(snf$period_stop, breaks=500)
first.50.obs <- snf[snf$period_obs < 50,]
first.50.stop <- snf[snf$period_stop < 50,]
par(mfrow=c(4,1))
hist(snf$height, breaks=100, main="Histogram of Heights", xlab="Height (in inches)")
hist(snf$weight, breaks=100, main="Histogram of Weight", xlab="weight (in lbs)")
hist(snf$period_obs, breaks=500)
hist(snf$period_stop, breaks=500)
lessthan.40.obs <- snf[snf$period_obs < 40,]
lessthan.40.stop <- snf[snf$period_stop < 40,]
jittered(scatterplot(lessthan.40.obs, lessthan.40.stop))
plot(jittered(lessthan.40.obs), lessthan.40.stop)
plot(jitter(lessthan.40.obs), lessthan.40.stop)
plot(jitter(lessthan.40.obs$period_obs), lessthan.40.stop$period_stop)
dim(lessthan.40.stop$period_stop
)
dim(lessthan.40.stop$period_stop)
head(lessthan.40.stop$period_stop)
type(lessthan.40.stop$period_stop)
typeof(lessthan.40.stop$period_stop)
max(lessthan.40.stop$period_stop)
min(lessthan.40.stop$period_stop)
dim(as.vector(lessthan.40.stop$period_stop))
dim(as.table(lessthan.40.stop$period_stop))
dim(as.data.frame(lessthan.40.stop$period_stop))
dim(snf)
lessthan.40 <- snf[snf$period.obs < 40,]
dim(lessthan.40)
max(snf[snf$period_obs])
max(snf[snf$period_obs,])
lessthan.40 <- snf[snf$period_obs < 40,]
dime(lessthan.40)
dim(lessthan.40)
plot(jitter(lessthan.40$period_obs), lessthan40$period_stop)
plot(jitter(lessthan.40$period_obs), lessthan.40$period_stop)
max(lessthn.40$period_stop)
max(lessthan.40$period_stop)
lessthan.40 <- snf[(snf$period_obs & snf$period_stop) < 40 ]
lessthan.40 <- snf[(snf$period_obs & snf$period_stop) < 40, ]
lessthan.40 <- snf[(snf$period_obs & snf$period_stop) < 40, ]
dim(lessthan.40)
max(lessthan.40$period_sto)
max(lessthan.40$period_stop)
lessthan.40 <- snf[snf$period_obs < 40 & snf$period_stop < 40, ]
dim(lessthan.40)
max(lesstan.40$period_stop)
max(lessthan.40$period_stop)
max(lessthan.40$period_obs)
ltfobs <- lessthan.40$period_obs
ltfst <- lessthan.40$period_stop
plot(jitter(ltfobs), ltfst)
plot(jitter(ltfobs), ltfst)
plot(jitter(ltfobs), ltfst, Main="Scatterplot of Time Observed vs Time Stopped", xlab="Time Observed", ylab="Time Stopped")
plot(jitter(ltfobs), ltfst, main="Scatterplot of Time Observed vs Time Stopped", xlab="Time Observed", ylab="Time Stopped")
lm(ltfobs, ltfst)
linear.model(ltfsobs, ltfst)
?lm
lm(ltfst ~ ltfobs)
linear.model <- lm(ltfst ~ ltfobs)
summar(linear.model)
summary(linear.model)
abline(linear.model)
slope <- linear.model$coefficients[[2]]
intercept <- linear.model$coefficients[[1]]
obs.time <- 5
prediction.5mins <- obs.time*slope + intercepts
prediction.5mins <- obs.time*slope + intercept
prediction.5mins
obs.time <- 60
prediction <- obs.time*slope + intercept
prediction
prediction.5mins <- obs.time*slope + intercept
obs.time <- 5
prediction <- obs.time*slope + intercept
prediction
obs.time <- 60
prediction
prediction <- obs.time*slope + intercept
prediction
summary(linear.model)
max(snf$height)
max(snf$height)
height <- snf$height
max(height)
95/12
.916667 * 12
weight <- snf$weight
max(weight)
plot(weight)
hist(weight)
hist(weight, breaks=100)
min(weight)
min(height)
reasonable <- snf[snf$height < 84 & (snf$weight > 80 & snf$weight < 400), ]
reasonable.weight <- reasonable$weight
reasonable.height <- reasonable$height
plot(jitter(reasonable$weight), reasonable$height, main="Scatterplot SNF Heights vs Weights", xlab="Weight", ylab="Height")
linear.model <- (reasonalbe$weight ~ reasonable$height)
abline(linear.model)
linear.model <- (reasonalbe$weight ~ reasonable$height)
summary(linear.model)
linear.model <- lm(reasonalbe$weight ~ reasonable$height)
linear.model <- lm(reasonable$weight ~ reasonableheight)
linear.model <- lm(reasonable$weight ~ reasonable$height)
abline(linear.model)
summary(linear.model)
linear.model
linear.model <- lm(reasonable$weight ~ reasonable$height)
lm <-lm(reasonable$weight ~ reasonable$height)
lm
lm <-lm(reasonable$height ~ reasonable$weight)
lm
abline(lm)
slope <- lm$coefficients[[2]]
intercept <- lm$coefficients[[1]]
obs.height <- 72
prediction <- obs.height*slope + intercept
prediction
lm <-lm(reasonable$weight ~ reasonable$height)
slope <- lm$coefficients[[2]]
intercept <- lm$coefficients[[1]]
prediction <- obs.height*slope + intercept
prediction
install.package(randomForest)
package.install(randomForest)
install.packages(randomForest)
install.packages('randomForest'')
install.packages('randomForest')
install.packages(randomForest)
library(randomForest)
?randomForest
head(snf)
fit <- randomForest(arrested ~ frisked + precinct + sex + race + age + period_obs + period_stop + period_obs + height + weight)
fit <- randomForest(snf$arrested ~ snf$frisked + snf$searched + snf$precinct + snf$sex + snf$race + snf$age + snf$period_obs + snf$period_stop + snf$period_obs + snf$height + snf$weight, data=snf)
print(fit)
fit <- randomForest(snf$arrested ~ snf$frisked + snf$searched + snf$precinct + snf$sex + snf$race + snf$age + snf$period_obs + snf$period_stop + snf$period_obs + snf$height + snf$weight, data=snf)
fit <- randomForest(snf$arrested ~ frisked + searched + precinct + sex + race + age + period_obs + period_stop + period_obs + height + weight, data=snf)
fit <- randomForest(arrested ~ frisked + searched + precinct + sex + race + age + period_obs + period_stop + period_obs + height + weight, data=snf)
pairs(snf)
library(rpart)
fit <- rpart(arrested ~ frisked + searched + precinct + sex + race + age + period_obs + period_stop + period_obs + height + weight, data=snf, method="class")
plot(fit)
printcp(fit)
plot(fit, uniform=TRUE,
main="Classification Tree for snf")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
tweets <- read.csv(file="~/Desktop/ccs/balacera/assets/balacera.csv")
text <- tweet$text
text <- tweets$text
typeof(text)
head(text)
text.corp <- Corpus(VectorSource(text))
lib(tm)
library(tm)
text.corp <- Corpus(VectorSource(text))
dim(text.corp)
head(text.corp)
text.corp[1:10]
text <- text.corp[1:100]
test <- text.corp[1:100]
test <- tm_map(test, stripWhitespace)
test <- tm_map(test, tolower)
test <- tm_map(test, removeWords, stopwords("spanish"))
test[1]
dtm <- DocumentTermMatrix(test, control = params)
dtm <- DocumentTermMatrix(test)
head(dtm)
dtm.mat <- as.matrix(dtm)
d <- dist(dtm.mat, method="euclidian")
fit <- hclust(d, method="Ward")
fit <- hclust(d, method="ward")
plot(fit)
test <- text.corp[1:1000]
test <- tm_map(test, stripWhitespace)
test <- tm_map(test, tolower)
test <- tm_map(test, removeWords, stopwords("spanish"))
#turn into DocumentTermMatrix
dtm <- DocumentTermMatrix(test)
dtm.mat <- as.matrix(dtm)
d <- dist(dtm.mat, method="euclidian")
fit <- hclust(d, method="ward")
plot(fit)
library(tm)
tweets <- read.csv(file="~/Desktop/ccs/balacera/assets/data/balacera.csv")
stopwords <- c(stopwords('spanish'), 'rt')
#establish the corpus
text.corp <- Corpus(VectorSource(tweets$text))
#preprocess the corpus
test <- text.corp[1:1000]
test <- tm_map(test, stripWhitespace)
test <- tm_map(test, tolower)
test <- tm_map(test, removePunctuation)
test <- tm_map(test, removeWords, stopwords)
dtm <- DocumentTermMatrix(test)
dtm2 <- removeSparseTerms(dtm, sparse=0.95)
df <- as.data.frame(inspect(dtm2))
df.scale <- scale(df)
#find euclidian distance
d <- dist(df.scale, method = "euclidean")
fit <- hclust(d, method="ward")
plot(fit)
groups <- cutree(fit, k=7)
rect.hclust(fit, k=5, border="red")
clusters <- rect.hclust(fit, k=5, border="red")
cluster.df <- as.data.frame((clusters[[1]]))
cluster.vec <- as.vector(cluster.df[,])
inspect(cluster[cluster.vec])
inspect(clusters[cluster.vec])
inspect(dtm[cluster.vec])
inspect(text.corp[cluster.vec])
findFreqTerms(text.corp[cluster.vec], lowfreq=30)
findFreqTerms(dtm[cluster.vec], lowfreq=30)
findFreqTerms(dtm[cluster.vec], lowfreq=10)
cluster.vec <- as.vector(cluster.df[,])
findFreqTerms(text.corp[cluster.vec], lowfreq=30)
cluster.df <- as.data.frame((clusters[[2]]))
cluster.vec <- as.vector(cluster.df[,])
#inspect tweets
inspect(cluster[cluster.vec])
cluster.df <- as.data.frame((clusters[[2]]))
cluster.vec <- as.vector(cluster.df[,])
#inspect tweets
inspect(text.corp[cluster.vec])
cluster.df <- as.data.frame((clusters[[3]]))
cluster.vec <- as.vector(cluster.df[,])
#inspect tweets
inspect(text.corp[cluster.vec])
cluster.df <- as.data.frame((clusters[[4]]))
cluster.vec <- as.vector(cluster.df[,])
#inspect tweets
inspect(text.corp[cluster.vec])
cities.pattern <- "Ju[aá]rez|Culiac[aá]n|Tijuana|Chihuahua|Acapulco\s+de\s+Ju[aá]rez|G[oó]mez|Palacio|Torre[oó]n|Mazatl[aá]n|Nogales|Durango|Navolato|Monterrey|Morelia|Ahome|Tepic|Reynosa|Guasave|Hidalgo\s+del\s+Parral|Ecatepec\s+de\s+Morelos|Uruapan"
# cities.pattern <- "Ju[aá]rez|Culiac[aá]n|Tijuana|Chihuahua|Acapulco\\s+de\\s+Ju[aá]rez|G[oó]mez|Palacio|Torre[oó]n|Mazatl[aá]n|Nogales|Durango|Navolato|Monterrey|Morelia|Ahome|Tepic|Reynosa|Guasave|Hidalgo\\s+del\\s+Parral|Ecatepec\\s+de\\s+Morelos|Uruapan"
# cities.pattern <- "Ju[aá]rez|Culiac[aá]n|Tijuana|Chihuahua|Acapulco\\s+de\\s+Ju[aá]rez|G[oó]mez|Palacio|Torre[oó]n|Mazatl[aá]n|Nogales|Durango|Navolato|Monterrey|Morelia|Ahome|Tepic|Reynosa|Guasave|Hidalgo\\s+del\\s+Parral|Ecatepec\\s+de\\s+Morelos|Uruapan/i"
?grep
?tm_filter
tm_filter(text.corpus, cities.pattern)
tm_filter(text.corp, cities.pattern)
cities.pattern <- "Ju[aá]rez|Culiac[aá]n|Tijuana|Chihuahua|Acapulco\\s+de\\s+Ju[aá]rez|G[oó]mez|Palacio|Torre[oó]n|Mazatl[aá]n|Nogales|Durango|Navolato|Monterrey|Morelia|Ahome|Tepic|Reynosa|Guasave|Hidalgo\\s+del\\s+Parral|Ecatepec\\s+de\\s+Morelos|Uruapan/i"
tm_filter(text.corpus, cities.pattern)
tm_filter(text.corp, cities.pattern)
cities <- tm_filter(text.corp, cities.pattern, ignore.case=TRUE)
cities <- tm_filter(text.corp, grep(cities.pattern, text.corp, ignore.case=TRUE))
CITIES
cities
text.corp
inspect(cities[1:20])
stopwords <- c(stopwords('spanish'), 'rt')
cities <- tm_map(cities, removeWords, stopwords)
cities <- tm_map(cities, removePunctuation)
cities <- tm_map(cities, tolower)
cities <- tm_map(cities, stripWhitespace)
cities.dtm <- DocumentTermMatrix(cities)
cities.dtm <- removeSparseTerms(cities.dtm, sparse=0.95)
citites.df <- as.data.frame(inspect(cities.dtm))
cities.scale <- scale(cities.df)
cities.scale <- scale(citites.df)
cities.d <- dist(cities.scale, method="euclidian")
cities.fit <- hclust(cities.d, method="ward")
plot(cities.fit)
cities.groups <- cutree(cities.fit, k=7)
clusters <- rect.hclust(fit, k=7, border="red")
rect.hclust(fit, k=7, border="red")
cluster.df <- as.data.frame((clusters[[1]]))
cluster.vec <- as.vector(cluster.df[,])
inspect(cluster[cluster.vec])
inspect(cities[cluster.vec])
patter <- "Ju[aá]rez""
patter <- "Ju[aá]rez"
pattern <- "Ju[aá]rez"
cities <- tm_filter(cities, grep(pattern, cities, ignore.case=TRUE))
cities
cities <- tm_filter(cities, grep("reynosa", cities, ignore.case=TRUE))
cities
cluster.df <- as.data.frame((clusters[[2]]))
cluster.vec <- as.vector(cluster.df[,])
inspect(cities[cluster.vec])
hist(tweets$created_at_seconds, breaks=1000, xlab="time", main="Histogram of Tweet Creation Time")
tweets <- read.csv(file="~/Desktop/ccs/balacera/assets/data/balacera-nov.csv")
hist(tweets$created_at_seconds, breaks=1000, xlab="time", main="Histogram of Tweet Creation Time")
sleep <- read.csv("~/Desktop/visuazing_js/week_10/sleep.csv")
head(sleep)
plot(sleep$timeInDeep, type="bar")
plot(sleep$timeInDeep, type="b")
plot(sleep$timeInDeep, type="l")
setwd("~/Desktop/qmss/spring2016/Stat4701/EDAV_HW1/")
survey <- read.csv("Survey+Response.csv")
survey
head(survey)
#naming vars and removing empty columns
names(survey) <- c("waitlist","program","tools","exp.Rmodeling","b5","b6","b7","b8","b9","b10","b11","gender","primaryeditor","exp.Rgraphics","exp.Radvanced","exp.documentation","exp.Matlab","exp.Github","b19","b20","b21","b22","b23","b24","b25","b26","b27","b28","b29","b30","b31","b32","b33","b34","b35","b36","b37","b38")
survey <- survey[,c(-5:-11,-19:-38)]
#dummy variables for each language/tool in tools
tooldummies = c()
toolList <- c("Github","Excel","SQL","RStudio","ggplot2","shell", "C/C","Python","LaTeX","(grep)","Sweave/knitr","XML","Web: html css js","dropbox","google drive","SPSS","Stata")
for(t in toolList){
tooldummies <- cbind(tooldummies,grepl(t,survey$tools))
}
tooldummies <- cbind(tooldummies,(grepl("R,",survey$tools)==TRUE | (grepl("R",survey$tools)==TRUE & grepl("RStudio",survey$tools)==FALSE)))
colnames(tooldummies) <- c("GitHub","Excel","SQL","RStudio","ggplot2","shell", "C","Python","LaTeX","grep","Sweave","XML","Web","dropbox","googledrive","SPSS","Stata","R")
survey <- cbind(survey,tooldummies)
head(sruvey)
head(survey)
head(survey)
summary(survey)
par(las = 2)
par(las = 2)
par(mar=c(5,5,5,5))
barplot(apply(survey[,12:29],2,mean),ylab = "Proportion of Class",ylim = c(0,1))
par(las = 2)
par(mar=c(5,5,5,5))
barplot(apply(survey[,12:29],2,mean),ylab = "Proportion of Class",ylim = c(0,1))
library(corrplot)
install.packages('corrplot')
library(corrplot)
corrplot(cor(survey[,12:29]),method='square')
survey$C
names(survey)
corrplot(cor(survey[,12:29]),method='square')
table(survey$program)
